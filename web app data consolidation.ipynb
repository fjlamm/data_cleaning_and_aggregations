{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking web app data against source\n",
    "\n",
    "we have a dashboard on our web app that gets the data from our reporting database. However, that reporting database was recently populated with data migrated from legacy systems. \n",
    "\n",
    "We want to verify that the dashboard and the database are showing the correct data, so we will need to verify it against an extract of the data from the legacy system on a csv or xlsx file.\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after familiarizing myself with the file I know I only need some of the columns and not all of them\n",
    "columns = ['IndividualCampus','Course','AttendedDate','MonthofClass','YearofClass']\n",
    "df = pd.read_excel('###.xlsx',sheet_name='Data',usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also rename the columns\n",
    "names = {'IndividualCampus':'campus','Course':'class','AttendedDate':'date','MonthofClass':'month','YearofClass':'year'}\n",
    "df.rename(names, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard is summarized by year and by campus. It also shows data only for last three years.\n",
    "\n",
    "So, let's use a df pivot to ageregate by campus and also for all campus per year. Let's also filter the dataframe for just the last three years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['year'].isin([2017,2018,2019])].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_101 = data[data['class'].str.contains('101')].groupby(by=['campus','year'],as_index=False)['date'].count().rename({'date':'total'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_101.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now bring in these values from the database\n",
    "import sqlalchemy\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '####'\n",
    "database = '####'\n",
    "username = '####'\n",
    "password = '######'\n",
    "driver= '{ODBC Driver 17 for SQL Server}'\n",
    "cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433'+';DATABASE='+database+';UID='+username+';PWD='+password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cnxn:\n",
    "    with cursor as crs:\n",
    "        string=('SELECT #####')\n",
    "    crs.execute(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_101 = pd.read_sql(string,con=cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_101.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now merge the two dataframes to compare\n",
    "compare_101 = data_101.merge(db_101,how='outer',left_on=['year','campus'],right_on=['year','CampusName'],sort=True)\n",
    "compare_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_101.fillna(0,inplace=True)\n",
    "compare_101.rename({'total_x':'total_xlsx','total_y':'total_database'},inplace=True, axis=1)\n",
    "compare_101['total_xlsx']=compare_101['total_xlsx'].astype(dtype='int64')\n",
    "compare_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences_101 = compare_101.copy()\n",
    "differences_101['diff']=differences_101['total_xlsx']-differences_101['total_database']\n",
    "differences_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add some color to ease the spotting of errors\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return 'color: %s' % color\n",
    "differences_101 = differences_101.style.applymap(color_negative_red,subset=pd.IndexSlice[:,'diff'])\n",
    "differences_101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "It seems like 38 out of 46 the results on the database are different from our source data.\n",
    "Let's investigate this a little bit more.\n",
    "I will:\n",
    "1. Verify that the source data is accurate\n",
    "2. Verify the database query to make sure I'm querying the right information\n",
    "3. Decide on next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Verify that the API call has the same results that we got with our query for 101 class numbers per campus\n",
    "2. If the first passes, Verify the data on the database to search for duplicates or other possible sources of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we got the API results on a excel file. Let's bring them in\n",
    "api_101 = pd.read_excel('api_class101.xlsx',sheet_name='class101',index_col=0)\n",
    "api_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see our dataframe from our query before\n",
    "db_101.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's pivot this dataframe to matc h the structure of our api df\n",
    "pivoted_101 = pd.pivot_table(db_101,values=['total'],index='CampusName',columns=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_101.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_101.columns = pivoted_101.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivoted_101.rename({'CampusName':'campus','2017':'2017','2018':'2018','2019':'2019'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pivoted_101.columns.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_vs_query = api_101.merge(pivoted_101,how='inner',left_on='campus',right_on='CampusName',suffixes=('_api','_query'))\n",
    "api_vs_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_vs_query.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's rename these columns\n",
    "api_vs_query.rename({2017:'2017_query',2018:'2018_query',2019:'2019_query'},inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_vs_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Since it's a very short list we can quickly see that there are no differences between our query and the api call.\n",
    "\n",
    "Given that these two are querying the same table on the database, and that the two qere created with different queries for the same purpose, and that both offer the same result, we can assume that the problem is not on the query itself but on the data.\n",
    "\n",
    "Therefore, we now proceed at looking at the actual data and finding duplicates or other possible sources of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create another query to the database to get all the rows without duplicate person ID's\n",
    "with cnxn:\n",
    "    with cursor as crs:\n",
    "        string=('SELECT #####')\n",
    "    crs.execute(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = pd.read_sql(string,con=cnxn)\n",
    "new_query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now join this with our data to check new differences if any\n",
    "compare_101_noDuplicates = compare_101.merge(new_query,how='outer',left_on=['year','campus'],right_on=['EventYear','CampusName'],sort=True)\n",
    "compare_101_noDuplicates.drop(['CampusName_x','CampusName_y','class','EventYear','total'],axis=1,inplace=True)\n",
    "compare_101_noDuplicates['diff'] = compare_101_noDuplicates['total_xlsx'] - compare_101_noDuplicates['noDuplicates']\n",
    "compare_101_noDuplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add some color to ease the spotting of errors\n",
    "def color_values(val):\n",
    "    if val < 0:\n",
    "        color = 'red'\n",
    "    elif val > 0:\n",
    "        color= 'green'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    return 'color: %s' % color\n",
    "colored_errors101 = compare_101_noDuplicates.style.applymap(color_values,subset=pd.IndexSlice[:,'diff'])\n",
    "colored_errors101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see, removing the duplicate personIds does help in some cases. In other cases it actually seems to have removed valid cases of a duplicate person ID.\n",
    "Also, even though we have removed duplicate person ID's we still have several instances with innaccurate data.\n",
    "\n",
    "Further investigation is required on the warehouse data and that is outside of the scope of the current research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('class101_errors.xlsx',engine='xlsxwriter')\n",
    "compare_101_noDuplicates.to_excel(writer,sheet_name='class101')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
