{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and agregation from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start by loading the load tab from the csv file with the data loaded into the warehouse\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.read_excel(io='####.xlsx',sheet_name='Load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df['Year'] = load_df['service_date'].dt.year\n",
    "load_df['Month'] = load_df['service_date'].dt.month\n",
    "load_df['woy'] = load_df['service_date'].dt.weekofyear\n",
    "load_df = load_df.rename({'Unnamed: 10':'dow'},axis=1)\n",
    "load_df['dow'] = load_df['service_date'].dt.dayofweek\n",
    "load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before we can start grouping we need to make sure that every day other than Sunday is assigned to the weekend of that immediate next sunday\n",
    "import datetime\n",
    "def set_weekend(row):\n",
    "    dow = row['dow']\n",
    "    date = row['service_date']\n",
    "    if dow != 6:\n",
    "        n = 6 - dow\n",
    "        date = (date + pd.Timedelta(days=n))\n",
    "    else:\n",
    "        date = date\n",
    "    return date\n",
    "weekend_dates = load_df.apply(set_weekend,axis=1)\n",
    "load_df['weekend_date'] = weekend_dates\n",
    "load_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's summarize this into monthly total averages for all campuses in US and then all internationals\n",
    "intl_list = ['Berlin','Buenos Aires','Manila','Hong Kong']\n",
    "load_us = load_df[~load_df['campus'].isin(intl_list)]\n",
    "load_in = load_df[load_df['campus'].isin(intl_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's begin with us campuses\n",
    "load_us_total = load_us[load_us['Year'].isin([2017,2018,2019])].groupby(['Year','Month']).agg({'attendance':np.sum,'weekend_date':'nunique'})#,('dow', lambda x: count(x) if x==1)]) \n",
    "load_us_total['Avg'] = (load_us_total['attendance']/load_us_total['weekend_date']).round(0).astype('int32')\n",
    "load_us_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now import the tables we summarized from the correct data\n",
    "us_reference_df = pd.read_excel(io='month averages from csv.xlsx',sheet_name='US',index_col='Year')\n",
    "in_reference_df = pd.read_excel(io='month averages from csv.xlsx', sheet_name = 'Intl')\n",
    "us_reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_reference_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_us_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's concatenate these two \n",
    "compare_us = load_us_total.merge(us_reference_df,left_on=['Year','Month'], right_on=['Year','Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_us = compare_us[compare_us['Avg_x']!=compare_us['Avg_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's dig some more on these differences. We know that jan 17 is missing some data from dec 16 \n",
    "#so we'll leave that one off, let's check the sep 17\n",
    "test1 = load_us[(load_us['Month']==9)&(load_us['Year']==2017)]\n",
    "test1['service_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['weekend_date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "We can see how some dates fall on a weekend that is moved to the next month. This is the case when you have an event on Saturday on the last of the month; that day is then assigned to the first weekend of the month.\n",
    "\n",
    "**Let's then add a service month column so we can group by that column rather than the actual service date month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df['weekend_month'] = load_df['weekend_date'].dt.month\n",
    "load_df['weekend_year'] = load_df['weekend_date'].dt.year\n",
    "load_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now group by this column rather than the month of the actual service and see how that affects the results\n",
    "#let's also filter by weekend_dateand see if that solves the issue with jan 2017\n",
    "load_us = load_df[~load_df['campus'].isin(intl_list)]\n",
    "load_us_total = load_us[load_us['weekend_date'].dt.year>2016].groupby(['weekend_year','weekend_month'],as_index=False).agg({'attendance':np.sum,'weekend_date':'nunique'})#,('dow', lambda x: count(x) if x==1)]) \n",
    "load_us_total['Avg'] = (load_us_total['attendance']/load_us_total['weekend_date']).round(0).astype('int32')\n",
    "load_us_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compare to the reference df\n",
    "compare_us = load_us_total.merge(us_reference_df,left_on=['weekend_year','weekend_month'], right_on=['Year','Month'])\n",
    "compare_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_us = compare_us[compare_us['Avg_x']!=compare_us['Avg_y']]\n",
    "errors_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have 0 errors now! that is great. This means that the data is correct after the first stage of transformation on Excel.\n",
    "Let's now check the data loaded into the warehouse to verify is working fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also do international campuses and add the two of them\n",
    "load_in = load_df[load_df['campus'].isin(intl_list)]\n",
    "load_in_total = load_in[load_in['weekend_date'].dt.year>2016].groupby(['weekend_year','weekend_month'],as_index=False).agg({'attendance':np.sum,'weekend_date':'nunique'})#,('dow', lambda x: count(x) if x==1)]) \n",
    "load_in_total['Avg'] = (load_in_total['attendance']/load_in_total['weekend_date']).round(0).astype('int32')\n",
    "load_in_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_in = load_in_total.merge(in_reference_df,left_on=['weekend_year','weekend_month'], right_on=['Year','Month'])\n",
    "compare_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_in = compare_in[compare_in['Avg_x']!=compare_in['Avg_y']]\n",
    "errors_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Include Volunteers\n",
    "\n",
    "It seems that the discrepancies between the load and the Intl tabs are due to the the volunteers. On the Intl tab, the volunteers were included in the count. They were not incldued on the Load tab. Let's bring in the volunteers count, add them to the international and see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_volunteers = pd.read_excel('month averages from csv.xlsx',sheet_name='Volunteers')\n",
    "in_volunteers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in = compare_in.merge(in_volunteers,left_on=['Year','Month'],right_on=['Year','Month'])\n",
    "total_in['TotalLoadWithVolunteers'] = total_in['Avg_x']+total_in['Avg']\n",
    "total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_in = total_in[total_in['Avg_y']!=total_in['TotalLoadWithVolunteers']]\n",
    "errors_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that the numbers are different for less than 0.001%. That is probably due to the rounding up on the calculations and is a very acceptable margin of error. Let's now do a Load Dataframe with all the DF results so we can later compare to the calculations from the Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_totals = load_us_total.merge(total_in[['weekend_year','weekend_month','Avg_x','TotalLoadWithVolunteers']],left_on=['weekend_year','weekend_month'],right_on=['weekend_year','weekend_month']).rename({'TotalLoadWithVolunteers':'IntlWithVolunteers'},axis=1)\n",
    "load_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_totals['All Campuses with Volunteers']=load_totals['Avg']+load_totals['IntlWithVolunteers']\n",
    "load_totals['All Campuses']=load_totals['Avg']+load_totals['Avg_x']\n",
    "\n",
    "load_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reference = us_reference_df.merge(in_reference_df,left_on=['Year','Month'],right_on=['Year','Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reference['Totals'] = total_reference['Avg_x']+total_reference['Avg_y']\n",
    "total_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all = total_reference.merge(load_totals[['weekend_year','weekend_month','All Campuses with Volunteers']],left_on=['Year','Month'],right_on=['weekend_year','weekend_month'])\n",
    "errors_all_campuses = compare_all[compare_all['Totals']!=compare_all['All Campuses with Volunteers']]\n",
    "errors_all_campuses['diff']=errors_all_campuses['Totals']-errors_all_campuses['All Campuses with Volunteers']\n",
    "errors_all_campuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Totals from Load Tab.xlsx',engine='xlsxwriter')\n",
    "load_totals.to_excel(writer,sheet_name = 'LoadTotals')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
